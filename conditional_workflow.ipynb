{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7654c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict , Annotated, Literal\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel, Field \n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2baccd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "27368d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= ChatGroq(model=\"openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e5a84f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm doing great, thank you for asking. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "res = model.invoke(\"Hello, how are you?\")\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bdfadbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentSchema(BaseModel):\n",
    "    sentiment: Literal['positive', 'negative'] = Field(description=\"The sentiment of the review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "af06e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagSchema(BaseModel):\n",
    "    issueType: Literal['UX', 'Performance', 'Bug', 'Other'] = Field(description=\"The type of issue mentioned in the review\")\n",
    "    tone: Literal['angry', 'sad', 'neutral', 'happy', 'other'] = Field(description=\"The tone of the review\")\n",
    "    urgency: Literal['low', 'medium', 'high'] = Field(description=\"The urgency of the issue mentioned in the review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "440cff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_model = model.with_structured_output(SentimentSchema)\n",
    "structured_model2 = model.with_structured_output(DiagSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b831e114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issueType='Bug' tone='neutral' urgency='medium'\n"
     ]
    }
   ],
   "source": [
    "res = structured_model2.invoke(\"There is something wrong with guidebook my goodbook is missing from product box.\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "338b7d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewState(TypedDict):\n",
    "    review: str\n",
    "    sentiment: Literal['positive', 'negative']\n",
    "    diagnosis: dict\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "de61e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(state: ReviewState) -> ReviewState:\n",
    "    prompt = f\"Determine if the following review is positive or negative: {state['review']}\"\n",
    "    sentiment = structured_model.invoke(prompt).sentiment\n",
    "    return {'sentiment': sentiment}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bca01e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentiment(state: ReviewState) -> Literal['positive_response', 'run_diagnosis']:\n",
    "\n",
    "    if state['sentiment'] == 'positive':\n",
    "        return 'positive_response'\n",
    "    else:\n",
    "        return 'run_diagnosis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "46425658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_response(state: ReviewState) :\n",
    "    prompt = f\"Generate a positive response wih warm thanks to the following review. Review: \\n{state['review']}\"\n",
    "    res = model.invoke(prompt)\n",
    "    return {'response': res.text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d90d8376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_diagnosis(state: ReviewState) :\n",
    "    prompt = f\"Analyze the following negative review and provide a diagnosis in terms of issue type, tone, and urgency. Review: \\n{state['review']}\"\n",
    "    diag = structured_model2.invoke(prompt)\n",
    "    \n",
    "    return {'diagnosis': diag.model_dump()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6d0d2885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_response(state: ReviewState) :\n",
    "    diagnosis = state['diagnosis']\n",
    "    prompt = f\"\"\"Generate a sympathetic response addressing the issues mentioned in the following review. Review: {state['review']} and '{diagnosis['issueType']}'issue with a '{diagnosis['tone']}' tone and '{diagnosis['urgency']}' urgency.\"\"\"\n",
    "    res = model.invoke(prompt)\n",
    "    return {'response': res.text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c645b7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m graph.add_edge(\u001b[33m'\u001b[39m\u001b[33mpositive_response\u001b[39m\u001b[33m'\u001b[39m, END)\n\u001b[32m     12\u001b[39m graph.add_edge(\u001b[33m'\u001b[39m\u001b[33mrun_diagnosis\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnegative_response\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_edge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnegative_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEND\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m workflow = graph.compile()\n\u001b[32m     17\u001b[39m workflow\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic AI\\Agentic-AI-using-LangGraph\\venv\\Lib\\site-packages\\langgraph\\graph\\state.py:615\u001b[39m, in \u001b[36mStateGraph.add_edge\u001b[39m\u001b[34m(self, start_key, end_key)\u001b[39m\n\u001b[32m    612\u001b[39m     \u001b[38;5;28mself\u001b[39m.edges.add((start_key, end_key))\n\u001b[32m    613\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstart_key\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mEND\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mraise\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;167;43;01mValueError\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEND cannot be a start node\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'function' object is not iterable"
     ]
    }
   ],
   "source": [
    "graph = StateGraph(ReviewState)\n",
    "graph.add_node('find_sentiment', find_sentiment)\n",
    "graph.add_node('positive_response', positive_response)\n",
    "graph.add_node('run_diagnosis', run_diagnosis)\n",
    "graph.add_node('negative_response', negative_response)\n",
    "\n",
    "\n",
    "\n",
    "graph.add_edge(START, 'find_sentiment')\n",
    "graph.add_conditional_edges('find_sentiment', check_sentiment)\n",
    "graph.add_edge('positive_response', END)\n",
    "graph.add_edge('run_diagnosis', 'negative_response')\n",
    "graph.add_edge('negative_response', END)\n",
    "\n",
    "\n",
    "workflow = graph.compile()\n",
    "workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c12ff53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "initial_state = {'review': \"The product was great and I loved it!\"}\n",
    "final_state = workflow.invoke(initial_state)\n",
    "print(final_state['sentiment'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
